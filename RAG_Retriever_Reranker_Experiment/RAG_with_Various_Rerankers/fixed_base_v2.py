"""
ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤ì— ë§ëŠ” ë¦¬ë­ì»¤ ê¸°ë³¸ í´ë˜ìŠ¤ë“¤
í‘œì¤€í™”ëœ íŒŒë¼ë¯¸í„°ì™€ ë°˜í™˜ í˜•ì‹ ì‚¬ìš©
"""
import os
import json
import numpy as np
from typing import List, Optional, Any, Dict
import re
from abc import ABC, abstractmethod
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStore

from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi


# í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í´ë˜ìŠ¤ë“¤
class BaseDocumentCompressor(ABC):
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ BaseDocumentCompressor"""
    
    @abstractmethod
    def compress_documents(
        self,
        documents: List[Document],
        query: str,
        callbacks: Optional[Any] = None,
    ) -> List[Document]:
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        pass


class SentenceTransformerEmbeddings(Embeddings):
    """ê³µí†µ ì„ë² ë”© í´ë˜ìŠ¤"""
    def __init__(self, model_name: str = "intfloat/multilingual-e5-large-instruct"):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = self.model.encode(texts)
        return embeddings.tolist()

    def embed_query(self, text: str) -> List[float]:
        query_text = f"query: {text}"
        embedding = self.model.encode(query_text)
        return embedding.tolist()


class NaiveVectorStore(VectorStore):
    """ì™„ì „íˆ ìˆ˜ì •ëœ NaiveVectorStore"""
    def __init__(self, documents: List[Document], embeddings: List[List[float]], embedding_function: Embeddings):
        self.documents = documents
        self._embeddings_matrix = np.array(embeddings, dtype=np.float32)
        self.embedding_function = embedding_function
        self._embeddings_matrix = self._embeddings_matrix / np.linalg.norm(self._embeddings_matrix, axis=1, keepdims=True)

    def add_texts(self, texts: List[str], metadatas: Optional[List[dict]] = None, **kwargs) -> List[str]:
        """ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„"""
        raise NotImplementedError("add_textsëŠ” í˜„ì¬ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def similarity_search_by_vector(self, embedding: List[float], k: int = 4, **kwargs) -> List[Document]:
        query_vector = np.array(embedding, dtype=np.float32)
        query_norm = query_vector / np.linalg.norm(query_vector)
        similarities = np.dot(self._embeddings_matrix, query_norm)
        top_k_indices = similarities.argsort()[::-1][:k]
        return [self.documents[i] for i in top_k_indices]

    def similarity_search(self, query: str, k: int = 4, **kwargs) -> List[Document]:
        query_embedding = self.embedding_function.embed_query(query)
        return self.similarity_search_by_vector(query_embedding, k, **kwargs)

    def similarity_search_with_score(self, query: str, k: int = 4, **kwargs):
        query_embedding = self.embedding_function.embed_query(query)
        query_vector = np.array(query_embedding, dtype=np.float32)
        query_norm = query_vector / np.linalg.norm(query_vector)
        similarities = np.dot(self._embeddings_matrix, query_norm)
        top_k_indices = similarities.argsort()[::-1][:k]
        results = []
        for idx in top_k_indices:
            doc = self.documents[idx]
            score = float(similarities[idx])
            results.append((doc, score))
        return results

    def as_retriever(self, search_kwargs: Optional[dict] = None, **kwargs):
        """VectorStoreë¥¼ Retrieverë¡œ ë³€í™˜"""
        search_kwargs = search_kwargs or {}
        
        class SimpleRetriever:
            """ê°„ë‹¨í•œ Retriever êµ¬í˜„"""
            def __init__(self, vectorstore, search_kwargs):
                self.vectorstore = vectorstore
                self.search_kwargs = search_kwargs
            
            def _get_relevant_documents(self, query: str) -> List[Document]:
                k = self.search_kwargs.get("k", 4)
                return self.vectorstore.similarity_search(query, k=k)
            
            def invoke(self, query: str) -> List[Document]:
                return self._get_relevant_documents(query)
            
            def get_candidate_documents(self, query: str) -> List[dict]:
                """ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ í›„ë³´ ë¬¸ì„œ ë°˜í™˜"""
                docs = self._get_relevant_documents(query)
                candidate_docs = []
                for doc in docs:
                    candidate_docs.append({
                        'doc_id': doc.metadata.get('source', 'unknown'),
                        'chunk_index': doc.metadata.get('chunk_index', 0),
                        'filename': doc.metadata.get('filename', 'unknown'),
                        'text': doc.page_content,
                        'score': 1.0  # ê¸°ë³¸ ì ìˆ˜
                    })
                return candidate_docs
        
        return SimpleRetriever(self, search_kwargs)

    @classmethod
    def from_texts(cls, texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs):
        """ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„"""
        raise NotImplementedError("from_textsëŠ” í˜„ì¬ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


class BaseReranker(ABC):
    """ìƒˆë¡œìš´ í‘œì¤€ ë¦¬ë­ì»¤ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, top_n: int = 10):
        self.top_n = top_n
    
    @abstractmethod
    def rerank_documents(
        self,
        query: str,
        candidate_documents: Optional[List[dict]] = None,
        **kwargs
    ) -> dict:
        """
        ë¬¸ì„œ ë¦¬ë­í‚¹ ë©”ì„œë“œ - í‘œì¤€ ì¸í„°í˜ì´ìŠ¤
        
        Args:
            query (str): ì‚¬ìš©ì ì§ˆë¬¸/ê²€ìƒ‰ì–´
            candidate_documents (Optional[List[dict]]): í›„ë³´ ë¬¸ì„œë“¤
                ê° ë¬¸ì„œëŠ” {'doc_id': str, 'chunk_index': int, 'filename': str, 'text': str, 'score': float} í˜•ì‹
            
        Returns:
            dict: {'retrieved_docs': [{'doc_id': str, 'chunk_index': int, 'score': float, 'filename': str, 'text': str}, ...]}
        """
        pass


class SimpleCompressionRetriever:
    """ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì§€ì›í•˜ëŠ” ì••ì¶• ë¦¬íŠ¸ë¦¬ë²„"""
    
    def __init__(self, base_retriever, reranker: BaseReranker):
        self.base_retriever = base_retriever
        self.reranker = reranker
    
    def invoke(self, query: str) -> List[Document]:
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ"""
        # ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
        result = self.search_and_rerank(query)
        
        # Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        documents = []
        for doc_info in result['retrieved_docs']:
            doc = Document(
                page_content=doc_info['text'],
                metadata={
                    'source': doc_info['doc_id'],
                    'chunk_index': doc_info['chunk_index'],
                    'filename': doc_info['filename']
                }
            )
            documents.append(doc)
        
        return documents
    
    def search_and_rerank(self, query: str) -> dict:
        """ìƒˆë¡œìš´ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤"""
        # ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ë¬¸ì„œ íšë“
        candidate_docs = self.base_retriever.get_candidate_documents(query)
        
        # ë¦¬ë­í‚¹ ìˆ˜í–‰
        result = self.reranker.rerank_documents(query, candidate_docs)
        
        return result


# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def _tokenize_ko(text: str) -> List[str]:
    """ê°„ë‹¨í•œ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €"""
    return re.findall(r"[\wê°€-í£]+", text.lower())


def load_embeddings_data(embeddings_file: str) -> tuple:
    """ì„ë² ë”© ë°ì´í„° ë¡œë“œ ê³µí†µ í•¨ìˆ˜"""
    print(f"ğŸ“‚ ì„ë² ë”© ë°ì´í„° ë¡œë“œ ì¤‘: {embeddings_file}")
    with open(embeddings_file, "r", encoding="utf-8") as f:
        chunk_data = json.load(f)
    
    documents = []
    embeddings_array = []
    
    for item in chunk_data:
        doc = Document(
            page_content=item["text"],
            metadata={
                "filename": item["filename"],
                "chunk_index": item["chunk_index"],
                "source": f"{item['filename']}_chunk_{item['chunk_index']}"
            }
        )
        documents.append(doc)
        embeddings_array.append(item["embedding"])
    
    print(f"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    print(f"ğŸ“„ ì²« ë²ˆì§¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°: {documents[0].page_content[:100]}...")
    
    return documents, embeddings_array


def setup_environment():
    """í™˜ê²½ ì„¤ì • ê³µí†µ í•¨ìˆ˜"""
    load_dotenv()
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = os.getenv("LANGSMITH_ENDPOINT", "")
    os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY", "")
    print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ")


def create_legal_prompt():
    """ë²•ë¥  ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë¶€ë™ì‚°ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ğŸ“‹ **ì°¸ê³  ë¬¸ì„œ:**
{context}

ğŸ“ **ë‹µë³€ ì§€ì¹¨:**
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ê´€ë ¨ ë²•ë ¹ ì¡°ë¬¸ì´ë‚˜ ì¡°í•­ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”
3. ë²•ë¥ ì  ê·¼ê±°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”
4. ë¬¸ì„œì—ì„œ ëª…í™•í•œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”
5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”

â“ **ì§ˆë¬¸:** {question}

ğŸ’¡ **ë‹µë³€:**"""
    )


def format_docs(docs):
    """ë¬¸ì„œ í¬ë§·íŒ… ê³µí†µ í•¨ìˆ˜"""
    formatted_docs = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
        content = doc.page_content.strip()
        formatted_docs.append(f"ğŸ“„ **ë¬¸ì„œ {i}** ({source})\n{content}")
    return "\n\n" + "\n\n".join(formatted_docs) + "\n\n"


def get_embeddings_file_path(current_file_path: str, embeddings_file: str = "output_chunks_with_embeddings.json") -> str:
    """ì„ë² ë”© íŒŒì¼ ê²½ë¡œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •"""
    if os.path.isabs(embeddings_file):
        return embeddings_file
    
    # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ì—ì„œ ìƒìœ„ë¡œ 2ë‹¨ê³„ ì˜¬ë¼ê°€ì„œ ì„ë² ë”© íŒŒì¼ ì°¾ê¸°
    script_dir = os.path.dirname(os.path.abspath(current_file_path))
    embeddings_path = os.path.join(script_dir, "..", "..", "output_chunks_with_embeddings.json")
    embeddings_path = os.path.normpath(embeddings_path)
    
    return embeddings_path


# BM25 ê´€ë ¨ í´ë˜ìŠ¤ë“¤
class BM25Reranker(BaseReranker, BaseDocumentCompressor):
    """ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” BM25 ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 12):
        super().__init__(top_n)
    
    def rerank_documents(
        self,
        query: str,
        candidate_documents: Optional[List[dict]] = None,
        **kwargs
    ) -> dict:
        if not candidate_documents:
            return {'retrieved_docs': []}
        
        # BM25 ì ìˆ˜ ê³„ì‚°
        corpus_tokens = [_tokenize_ko(doc['text']) for doc in candidate_documents]
        bm25 = BM25Okapi(corpus_tokens)
        query_tokens = _tokenize_ko(query)
        scores = bm25.get_scores(query_tokens)
        
        # ì ìˆ˜ì™€ ë¬¸ì„œë¥¼ í•¨ê»˜ ì •ë ¬
        scored_docs = []
        for i, doc in enumerate(candidate_documents):
            scored_docs.append({
                'doc_id': doc['doc_id'],
                'chunk_index': doc['chunk_index'],
                'score': float(scores[i]),
                'filename': doc['filename'],
                'text': doc['text']
            })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
        scored_docs.sort(key=lambda x: x['score'], reverse=True)
        
        return {'retrieved_docs': scored_docs[:self.top_n]}
    
    def compress_documents(self, documents: List[Document], query: str, callbacks=None) -> List[Document]:
        """LangChain BaseDocumentCompressor ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜"""
        if not documents:
            return []
        
        # Documentë¥¼ dict í˜•íƒœë¡œ ë³€í™˜
        candidate_docs = []
        for i, doc in enumerate(documents):
            candidate_docs.append({
                'doc_id': f'doc_{i}',
                'chunk_index': i,
                'filename': doc.metadata.get('source', 'unknown'),
                'text': doc.page_content
            })
        
        # BM25 ë¦¬ë­í‚¹ ìˆ˜í–‰
        result = self.rerank_documents(query, candidate_docs)
        
        # ê²°ê³¼ë¥¼ Documentë¡œ ë³€í™˜
        reranked_docs = []
        for doc_info in result['retrieved_docs']:
            # ì›ë³¸ ë¬¸ì„œì—ì„œ í•´ë‹¹í•˜ëŠ” Document ì°¾ê¸°
            for doc in documents:
                if doc.page_content == doc_info['text']:
                    reranked_docs.append(doc)
                    break
        
        return reranked_docs[:self.top_n]


# CrossEncoder ê´€ë ¨ í´ë˜ìŠ¤ë“¤
class SentenceTransformerRerank(BaseReranker, BaseDocumentCompressor):
    """ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” CrossEncoder ë¦¬ë­ì»¤"""
    
    def __init__(self, model_name: str, top_n: int = 10):
        super().__init__(top_n)
        try:
            from sentence_transformers import CrossEncoder
            self.model = CrossEncoder(model_name)
        except ImportError:
            raise ImportError("sentence_transformersê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sentence-transformers")
    
    def rerank_documents(
        self,
        query: str,
        candidate_documents: Optional[List[dict]] = None,
        **kwargs
    ) -> dict:
        if not candidate_documents:
            return {'retrieved_docs': []}
        
        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
        pairs = [[query, doc['text']] for doc in candidate_documents]
        scores = self.model.predict(pairs)
        
        # ì ìˆ˜ì™€ ë¬¸ì„œë¥¼ í•¨ê»˜ ì •ë ¬
        scored_docs = []
        for i, doc in enumerate(candidate_documents):
            scored_docs.append({
                'doc_id': doc['doc_id'],
                'chunk_index': doc['chunk_index'],
                'score': float(scores[i]),
                'filename': doc['filename'],
                'text': doc['text']
            })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
        scored_docs.sort(key=lambda x: x['score'], reverse=True)
        
        return {'retrieved_docs': scored_docs[:self.top_n]}
    
    def compress_documents(self, documents: List[Document], query: str, callbacks=None) -> List[Document]:
        """LangChain BaseDocumentCompressor ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜"""
        if not documents:
            return []
        
        # Documentë¥¼ dict í˜•íƒœë¡œ ë³€í™˜
        candidate_docs = []
        for i, doc in enumerate(documents):
            candidate_docs.append({
                'doc_id': f'doc_{i}',
                'chunk_index': i,
                'filename': doc.metadata.get('source', 'unknown'),
                'text': doc.page_content
            })
        
        # CrossEncoder ë¦¬ë­í‚¹ ìˆ˜í–‰
        result = self.rerank_documents(query, candidate_docs)
        
        # ê²°ê³¼ë¥¼ Documentë¡œ ë³€í™˜
        reranked_docs = []
        for doc_info in result['retrieved_docs']:
            # ì›ë³¸ ë¬¸ì„œì—ì„œ í•´ë‹¹í•˜ëŠ” Document ì°¾ê¸°
            for doc in documents:
                if doc.page_content == doc_info['text']:
                    reranked_docs.append(doc)
                    break
        
        return reranked_docs[:self.top_n]


# Embedding ê´€ë ¨ í´ë˜ìŠ¤ë“¤
class EmbeddingCosineReranker(BaseReranker):
    """ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 10, embed_model_name: str = "intfloat/multilingual-e5-large-instruct"):
        super().__init__(top_n)
        self.embed = SentenceTransformerEmbeddings(embed_model_name)
    
    def rerank_documents(
        self,
        query: str,
        candidate_documents: Optional[List[dict]] = None,
        **kwargs
    ) -> dict:
        if not candidate_documents:
            return {'retrieved_docs': []}
        
        # ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
        q = np.array(self.embed.embed_query(query), dtype=np.float32)
        ds = np.array(self.embed.embed_documents([doc['text'] for doc in candidate_documents]), dtype=np.float32)
        sims = np.dot(ds, q)
        
        # ì ìˆ˜ì™€ ë¬¸ì„œë¥¼ í•¨ê»˜ ì •ë ¬
        scored_docs = []
        for i, doc in enumerate(candidate_documents):
            scored_docs.append({
                'doc_id': doc['doc_id'],
                'chunk_index': doc['chunk_index'],
                'score': float(sims[i]),
                'filename': doc['filename'],
                'text': doc['text']
            })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
        scored_docs.sort(key=lambda x: x['score'], reverse=True)
        
        return {'retrieved_docs': scored_docs[:self.top_n]}


# Hybrid ê´€ë ¨ í´ë˜ìŠ¤ë“¤
class CombSumReranker(BaseReranker):
    """CombSum ë°©ì‹ì˜ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 12):
        super().__init__(top_n)
        self.embed_model = SentenceTransformerEmbeddings()
    
    def rerank_documents(
        self,
        query: str,
        candidate_documents: Optional[List[dict]] = None,
        **kwargs
    ) -> dict:
        if not candidate_documents:
            return {'retrieved_docs': []}
        
        # BM25 ì ìˆ˜ ê³„ì‚°
        corpus_tokens = [_tokenize_ko(doc['text']) for doc in candidate_documents]
        bm25 = BM25Okapi(corpus_tokens)
        query_tokens = _tokenize_ko(query)
        bm25_scores = bm25.get_scores(query_tokens)
        
        # ì„ë² ë”© ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        q_emb = np.array(self.embed_model.embed_query(query), dtype=np.float32)
        doc_embs = np.array(self.embed_model.embed_documents([d['text'] for d in candidate_documents]), dtype=np.float32)
        embed_scores = np.dot(doc_embs, q_emb)
        
        # ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„)
        bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-8)
        embed_scores = (embed_scores - embed_scores.min()) / (embed_scores.max() - embed_scores.min() + 1e-8)
        
        # CombSum: ë‘ ì ìˆ˜ë¥¼ í•©ì‚°
        combined_scores = bm25_scores + embed_scores
        
        # ì ìˆ˜ì™€ ë¬¸ì„œë¥¼ í•¨ê»˜ ì •ë ¬
        scored_docs = []
        for i, doc in enumerate(candidate_documents):
            scored_docs.append({
                'doc_id': doc['doc_id'],
                'chunk_index': doc['chunk_index'],
                'score': float(combined_scores[i]),
                'filename': doc['filename'],
                'text': doc['text']
            })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
        scored_docs.sort(key=lambda x: x['score'], reverse=True)
        
        return {'retrieved_docs': scored_docs[:self.top_n]}


# LLM ê´€ë ¨ í´ë˜ìŠ¤ë“¤
class LLMReranker(BaseReranker):
    """LLM ê¸°ë°˜ ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 10, llm_model: str = "gpt-4o-mini"):
        super().__init__(top_n)
        self.llm = ChatOpenAI(
            model=llm_model,
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY"),
        )
    
    def rerank_documents(
        self,
        query: str,
        candidate_documents: Optional[List[dict]] = None,
        **kwargs
    ) -> dict:
        if not candidate_documents or not os.getenv("OPENAI_API_KEY"):
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ìˆœì„œ ìœ ì§€
            return {'retrieved_docs': candidate_documents[:self.top_n] if candidate_documents else []}
        
        # LLMì„ ì‚¬ìš©í•œ ê´€ë ¨ì„± í‰ê°€
        scored_docs = []
        for doc in candidate_documents:
            prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ 0-10 ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ë¬¸ì„œ: {doc['text'][:1000]}

ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë‹µí•˜ì„¸ìš” (0-10):"""
            
            try:
                response = self.llm.invoke(prompt).content.strip()
                score = float(response) if response.replace('.', '').isdigit() else 5.0
            except:
                score = 5.0  # ê¸°ë³¸ ì ìˆ˜
            
            scored_docs.append({
                'doc_id': doc['doc_id'],
                'chunk_index': doc['chunk_index'],
                'score': score,
                'filename': doc['filename'],
                'text': doc['text']
            })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
        scored_docs.sort(key=lambda x: x['score'], reverse=True)
        
        return {'retrieved_docs': scored_docs[:self.top_n]}


# Rules ê´€ë ¨ í´ë˜ìŠ¤ë“¤
class LegalRuleBoostReranker(BaseReranker):
    """ë²•ë¥  ê·œì¹™ ê¸°ë°˜ ë¶€ìŠ¤íŠ¸ ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 12):
        super().__init__(top_n)
        self.embed_model = SentenceTransformerEmbeddings()
        
        # ë²•ë¥  ì¡°ë¬¸ íŒ¨í„´
        self._ARTICLE_RE = re.compile(r"ì œ\s?(\d+)\s?ì¡°")
        self._PARA_RE = re.compile(r"ì œ\s?(\d+)\s?í•­")
        self._ITEM_RE = re.compile(r"ì œ\s?(\d+)\s?í˜¸")
    
    def _calculate_legal_boost(self, text: str, query: str) -> float:
        """ë²•ë¥  ë¬¸ì„œì˜ ì¤‘ìš”ë„ ë¶€ìŠ¤íŠ¸ ê³„ì‚°"""
        boost = 0.0
        
        # ì¡°ë¬¸ ì–¸ê¸‰ ë¶€ìŠ¤íŠ¸
        if self._ARTICLE_RE.search(text):
            boost += 0.3
        
        # í•­ ì–¸ê¸‰ ë¶€ìŠ¤íŠ¸  
        if self._PARA_RE.search(text):
            boost += 0.2
            
        # í˜¸ ì–¸ê¸‰ ë¶€ìŠ¤íŠ¸
        if self._ITEM_RE.search(text):
            boost += 0.1
        
        # íŠ¹ì • í‚¤ì›Œë“œ ë¶€ìŠ¤íŠ¸
        legal_keywords = ["ë²•ë¥ ", "ì¡°ë¬¸", "ê·œì •", "ì¡°í•­", "ë¶€ë™ì‚°", "ì„¸ë²•", "ì¢…í•©ë¶€ë™ì‚°ì„¸"]
        for keyword in legal_keywords:
            if keyword in text:
                boost += 0.1
                
        # ì§ˆë¬¸ í‚¤ì›Œë“œì™€ì˜ ë§¤ì¹­ ë¶€ìŠ¤íŠ¸
        query_words = query.split()
        for word in query_words:
            if len(word) > 1 and word in text:
                boost += 0.05
        
        return min(boost, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    def rerank_documents(
        self,
        query: str,
        candidate_documents: Optional[List[dict]] = None,
        **kwargs
    ) -> dict:
        if not candidate_documents:
            return {'retrieved_docs': []}
        
        # ê¸°ë³¸ ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚°
        q_emb = np.array(self.embed_model.embed_query(query), dtype=np.float32)
        doc_embs = np.array(self.embed_model.embed_documents([d['text'] for d in candidate_documents]), dtype=np.float32)
        base_scores = np.dot(doc_embs, q_emb)
        
        # ë²•ë¥  ê·œì¹™ ë¶€ìŠ¤íŠ¸ ì ìš©
        scored_docs = []
        for i, doc in enumerate(candidate_documents):
            boost = self._calculate_legal_boost(doc['text'], query)
            boosted_score = base_scores[i] * (1 + boost)
            
            scored_docs.append({
                'doc_id': doc['doc_id'],
                'chunk_index': doc['chunk_index'],
                'score': float(boosted_score),
                'filename': doc['filename'],
                'text': doc['text']
            })
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
        scored_docs.sort(key=lambda x: x['score'], reverse=True)
        
        return {'retrieved_docs': scored_docs[:self.top_n]}


class EmbeddingCosineCompressor(BaseDocumentCompressor):
    """ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 10, model_name: str = "intfloat/multilingual-e5-large-instruct"):
        super().__init__()
        self.top_n = top_n
        self.embed_model = SentenceTransformerEmbeddings(model_name)
    
    def compress_documents(self, documents: List[Document], query: str, callbacks=None) -> List[Document]:
        if not documents:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = np.array(self.embed_model.embed_query(query))
        
        # ë¬¸ì„œë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        doc_scores = []
        for doc in documents:
            doc_embedding = np.array(self.embed_model.embed_documents([doc.page_content])[0])
            cosine_sim = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
            doc_scores.append((doc, cosine_sim))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [doc for doc, score in doc_scores[:self.top_n]]


# ë‹¤ë¥¸ ì„ë² ë”© ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•˜ëŠ” ë¦¬ë­ì»¤ë“¤
class EmbeddingCosineGTECompressor(BaseDocumentCompressor):
    """GTE ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 10):
        super().__init__()
        self.top_n = top_n
        self.embed_model = SentenceTransformerEmbeddings("sentence-transformers/gte-large")
    
    def compress_documents(self, documents: List[Document], query: str, callbacks=None) -> List[Document]:
        if not documents:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = np.array(self.embed_model.embed_query(query))
        
        # ë¬¸ì„œë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        doc_scores = []
        for doc in documents:
            doc_embedding = np.array(self.embed_model.embed_documents([doc.page_content])[0])
            cosine_sim = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
            doc_scores.append((doc, cosine_sim))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [doc for doc, score in doc_scores[:self.top_n]]


class EmbeddingCosineMPNetCompressor(BaseDocumentCompressor):
    """MPNet ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 10):
        super().__init__()
        self.top_n = top_n
        self.embed_model = SentenceTransformerEmbeddings("sentence-transformers/all-mpnet-base-v2")
    
    def compress_documents(self, documents: List[Document], query: str, callbacks=None) -> List[Document]:
        if not documents:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = np.array(self.embed_model.embed_query(query))
        
        # ë¬¸ì„œë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        doc_scores = []
        for doc in documents:
            doc_embedding = np.array(self.embed_model.embed_documents([doc.page_content])[0])
            cosine_sim = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
            doc_scores.append((doc, cosine_sim))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [doc for doc, score in doc_scores[:self.top_n]]


class EmbeddingCosineParaphraseCompressor(BaseDocumentCompressor):
    """Paraphrase ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 10):
        super().__init__()
        self.top_n = top_n
        self.embed_model = SentenceTransformerEmbeddings("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    
    def compress_documents(self, documents: List[Document], query: str, callbacks=None) -> List[Document]:
        if not documents:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = np.array(self.embed_model.embed_query(query))
        
        # ë¬¸ì„œë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        doc_scores = []
        for doc in documents:
            doc_embedding = np.array(self.embed_model.embed_documents([doc.page_content])[0])
            cosine_sim = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
            doc_scores.append((doc, cosine_sim))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [doc for doc, score in doc_scores[:self.top_n]]


class EmbeddingCosineStellaCompressor(BaseDocumentCompressor):
    """Stella ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë¦¬ë­ì»¤"""
    
    def __init__(self, top_n: int = 10):
        super().__init__()
        self.top_n = top_n
        self.embed_model = SentenceTransformerEmbeddings("infgrad/stella-base-ko-v2")
    
    def compress_documents(self, documents: List[Document], query: str, callbacks=None) -> List[Document]:
        if not documents:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = np.array(self.embed_model.embed_query(query))
        
        # ë¬¸ì„œë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        doc_scores = []
        for doc in documents:
            doc_embedding = np.array(self.embed_model.embed_documents([doc.page_content])[0])
            cosine_sim = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
            doc_scores.append((doc, cosine_sim))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [doc for doc, score in doc_scores[:self.top_n]]


def _tokenize_ko(text: str) -> List[str]:
    """í•œêµ­ì–´ í† í°í™” í•¨ìˆ˜"""
    import re
    # ê°„ë‹¨í•œ í•œêµ­ì–´ í† í°í™” (ê³µë°±, êµ¬ë‘ì  ê¸°ì¤€)
    tokens = re.findall(r'\b\w+\b', text.lower())
    return tokens