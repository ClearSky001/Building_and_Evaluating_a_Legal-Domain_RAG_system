"""
ëª¨ë“  ë¦¬ëž­ì»¤ë¥¼ ìœ„í•œ ì™„ì „ížˆ ìˆ˜ì •ëœ ê¸°ë³¸ í´ëž˜ìŠ¤ë“¤
LangChain í˜¸í™˜ì„± ë¬¸ì œë¥¼ ì™„ì „ížˆ í•´ê²°í•œ ë…ë¦½ì ì¸ êµ¬í˜„
"""
import os
import json
import numpy as np
from typing import List, Optional, Any
import re
from abc import ABC, abstractmethod
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStore

from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi


class SentenceTransformerEmbeddings(Embeddings):
    """ê³µí†µ ìž„ë² ë”© í´ëž˜ìŠ¤"""
    def __init__(self, model_name: str = "intfloat/multilingual-e5-large-instruct"):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = self.model.encode(texts)
        return embeddings.tolist()

    def embed_query(self, text: str) -> List[float]:
        query_text = f"query: {text}"
        embedding = self.model.encode(query_text)
        return embedding.tolist()


class NaiveVectorStore(VectorStore):
    """ì™„ì „ížˆ ìˆ˜ì •ëœ NaiveVectorStore"""
    def __init__(self, documents: List[Document], embeddings: List[List[float]], embedding_function: Embeddings):
        self.documents = documents
        self._embeddings_matrix = np.array(embeddings, dtype=np.float32)
        self.embedding_function = embedding_function
        self._embeddings_matrix = self._embeddings_matrix / np.linalg.norm(self._embeddings_matrix, axis=1, keepdims=True)

    def add_texts(self, texts: List[str], metadatas: Optional[List[dict]] = None, **kwargs) -> List[str]:
        """ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„"""
        raise NotImplementedError("add_textsëŠ” í˜„ìž¬ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def similarity_search_by_vector(self, embedding: List[float], k: int = 4, **kwargs) -> List[Document]:
        query_vector = np.array(embedding, dtype=np.float32)
        query_norm = query_vector / np.linalg.norm(query_vector)
        similarities = np.dot(self._embeddings_matrix, query_norm)
        top_k_indices = similarities.argsort()[::-1][:k]
        return [self.documents[i] for i in top_k_indices]

    def similarity_search(self, query: str, k: int = 4, **kwargs) -> List[Document]:
        query_embedding = self.embedding_function.embed_query(query)
        return self.similarity_search_by_vector(query_embedding, k, **kwargs)

    def similarity_search_with_score(self, query: str, k: int = 4, **kwargs):
        query_embedding = self.embedding_function.embed_query(query)
        query_vector = np.array(query_embedding, dtype=np.float32)
        query_norm = query_vector / np.linalg.norm(query_vector)
        similarities = np.dot(self._embeddings_matrix, query_norm)
        top_k_indices = similarities.argsort()[::-1][:k]
        results = []
        for idx in top_k_indices:
            doc = self.documents[idx]
            score = float(similarities[idx])
            results.append((doc, score))
        return results

    def as_retriever(self, search_kwargs: Optional[dict] = None, **kwargs):
        """VectorStoreë¥¼ Retrieverë¡œ ë³€í™˜ - ì™„ì „ížˆ ë…ë¦½ì ì¸ êµ¬í˜„"""
        search_kwargs = search_kwargs or {}
        
        class SimpleRetriever:
            """ê°„ë‹¨í•œ Retriever êµ¬í˜„"""
            def __init__(self, vectorstore, search_kwargs):
                self.vectorstore = vectorstore
                self.search_kwargs = search_kwargs
            
            def _get_relevant_documents(self, query: str) -> List[Document]:
                k = self.search_kwargs.get("k", 4)
                return self.vectorstore.similarity_search(query, k=k)
            
            def invoke(self, query: str) -> List[Document]:
                return self._get_relevant_documents(query)
        
        return SimpleRetriever(self, search_kwargs)

    @classmethod
    def from_texts(cls, texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs):
        """ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„"""
        raise NotImplementedError("from_textsëŠ” í˜„ìž¬ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


class BaseDocumentCompressor(ABC):
    """ì™„ì „ížˆ ë…ë¦½ì ì¸ BaseDocumentCompressor"""
    
    def __init__(self):
        pass
    
    @abstractmethod
    def compress_documents(
        self,
        documents: List[Document],
        query: str,
        callbacks: Optional[Any] = None,
    ) -> List[Document]:
        """ë¬¸ì„œ ì••ì¶•/ë¦¬ëž­í‚¹ ë©”ì„œë“œ"""
        pass


class SimpleCompressionRetriever:
    """ê°„ë‹¨í•œ ì••ì¶• ë¦¬íŠ¸ë¦¬ë²„ - ContextualCompressionRetriever ì™„ì „ ëŒ€ì²´"""
    
    def __init__(self, base_retriever, compressor):
        self.base_retriever = base_retriever
        self.compressor = compressor
    
    def invoke(self, query: str) -> List[Document]:
        """ë¬¸ì„œ ê²€ìƒ‰ ë° ë¦¬ëž­í‚¹"""
        # ê¸°ë³¸ ê²€ìƒ‰
        documents = self.base_retriever._get_relevant_documents(query)
        
        # ë¦¬ëž­í‚¹
        if self.compressor and documents:
            documents = self.compressor.compress_documents(documents, query)
        
        return documents


# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def _tokenize_ko(text: str) -> List[str]:
    """ê°„ë‹¨í•œ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €"""
    return re.findall(r"[\wê°€-íž£]+", text.lower())


def load_embeddings_data(embeddings_file: str) -> tuple:
    """ìž„ë² ë”© ë°ì´í„° ë¡œë“œ ê³µí†µ í•¨ìˆ˜"""
    print(f"ðŸ“‚ ìž„ë² ë”© ë°ì´í„° ë¡œë“œ ì¤‘: {embeddings_file}")
    with open(embeddings_file, "r", encoding="utf-8") as f:
        chunk_data = json.load(f)
    
    documents = []
    embeddings_array = []
    
    for item in chunk_data:
        doc = Document(
            page_content=item["text"],
            metadata={
                "filename": item["filename"],
                "chunk_index": item["chunk_index"],
                "source": f"{item['filename']}_chunk_{item['chunk_index']}"
            }
        )
        documents.append(doc)
        embeddings_array.append(item["embedding"])
    
    print(f"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    print(f"ðŸ“„ ì²« ë²ˆì§¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°: {documents[0].page_content[:100]}...")
    
    return documents, embeddings_array


def setup_environment():
    """í™˜ê²½ ì„¤ì • ê³µí†µ í•¨ìˆ˜"""
    load_dotenv()
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = os.getenv("LANGSMITH_ENDPOINT", "")
    os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY", "")
    print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ")


def create_legal_prompt():
    """ë²•ë¥  ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë¶€ë™ì‚°ì„¸ë²• ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ðŸ“‹ **ì°¸ê³  ë¬¸ì„œ:**
{context}

ðŸ“ **ë‹µë³€ ì§€ì¹¨:**
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ê´€ë ¨ ë²•ë ¹ ì¡°ë¬¸ì´ë‚˜ ì¡°í•­ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”
3. ë²•ë¥ ì  ê·¼ê±°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”
4. ë¬¸ì„œì—ì„œ ëª…í™•í•œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”
5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ì„¸ìš”

â“ **ì§ˆë¬¸:** {question}

ðŸ’¡ **ë‹µë³€:**"""
    )


def format_docs(docs):
    """ë¬¸ì„œ í¬ë§·íŒ… ê³µí†µ í•¨ìˆ˜"""
    formatted_docs = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
        content = doc.page_content.strip()
        formatted_docs.append(f"ðŸ“„ **ë¬¸ì„œ {i}** ({source})\n{content}")
    return "\n\n" + "\n\n".join(formatted_docs) + "\n\n"


def get_embeddings_file_path(current_file_path: str, embeddings_file: str = "output_chunks_with_embeddings.json") -> str:
    """ìž„ë² ë”© íŒŒì¼ ê²½ë¡œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •"""
    if os.path.isabs(embeddings_file):
        return embeddings_file
    
    # í˜„ìž¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ì—ì„œ ìƒìœ„ë¡œ 2ë‹¨ê³„ ì˜¬ë¼ê°€ì„œ ìž„ë² ë”© íŒŒì¼ ì°¾ê¸°
    script_dir = os.path.dirname(os.path.abspath(current_file_path))
    embeddings_path = os.path.join(script_dir, "..", "..", "output_chunks_with_embeddings.json")
    embeddings_path = os.path.normpath(embeddings_path)
    
    return embeddings_path


# BM25 ê´€ë ¨ í´ëž˜ìŠ¤ë“¤
class BM25Reranker(BaseDocumentCompressor):
    """ë…ë¦½ì ì¸ BM25 ë¦¬ëž­ì»¤"""
    def __init__(self, top_n: int = 5):
        super().__init__()
        self.top_n = top_n

    def compress_documents(self, documents: List[Document], query: str, callbacks=None) -> List[Document]:
        if not documents:
            return []
        
        corpus_tokens = [_tokenize_ko(doc.page_content) for doc in documents]
        bm25 = BM25Okapi(corpus_tokens)
        query_tokens = _tokenize_ko(query)
        scores = bm25.get_scores(query_tokens)
        ranked = sorted(zip(scores, documents), key=lambda x: x[0], reverse=True)
        return [doc for _, doc in ranked[:self.top_n]]


# CrossEncoder ê´€ë ¨ í´ëž˜ìŠ¤ë“¤
class SentenceTransformerRerank(BaseDocumentCompressor):
    """ë…ë¦½ì ì¸ SentenceTransformerRerank"""
    
    def __init__(self, model_name: str, top_n: int = 10):
        super().__init__()
        try:
            from sentence_transformers import CrossEncoder
            self.model = CrossEncoder(model_name)
        except ImportError:
            raise ImportError("sentence_transformersê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sentence-transformers")
        self.top_n = top_n
    
    def compress_documents(
        self,
        documents: List[Document],
        query: str,
        callbacks: Optional[Any] = None,
    ) -> List[Document]:
        if not documents:
            return []
        
        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
        pairs = [[query, doc.page_content] for doc in documents]
        scores = self.model.predict(pairs)
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        doc_scores = list(zip(documents, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ Nê°œ ë°˜í™˜
        return [doc for doc, score in doc_scores[:self.top_n]]


# Embedding ê´€ë ¨ í´ëž˜ìŠ¤ë“¤
class EmbeddingCosineCompressor(BaseDocumentCompressor):
    """ë…ë¦½ì ì¸ EmbeddingCosineCompressor"""
    def __init__(self, top_n: int = 5, embed_model_name: str = "intfloat/multilingual-e5-large-instruct"):
        super().__init__()
        self.top_n = top_n
        self.embed = SentenceTransformerEmbeddings(embed_model_name)

    def compress_documents(self, documents: List[Document], query: str, callbacks=None) -> List[Document]:
        if not documents:
            return []
        q = np.array(self.embed.embed_query(query), dtype=np.float32)
        ds = np.array(self.embed.embed_documents([d.page_content for d in documents]), dtype=np.float32)
        sims = np.dot(ds, q)
        order = sims.argsort()[::-1][:self.top_n]
        return [documents[i] for i in order]
