"""
ëª¨ë“  ë¦¬ë­ì»¤ì—ì„œ ì‚¬ìš©í•  ê³µí†µ import ë° ê¸°ë³¸ í´ë˜ìŠ¤ë“¤
LangChain ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œë¥¼ ì™„ì „íˆ í•´ê²°í•˜ê¸° ìœ„í•œ ë…ë¦½ì ì¸ êµ¬í˜„
"""
import os
import json
import numpy as np
from typing import List, Optional, Any
from abc import ABC, abstractmethod

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStore
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from sentence_transformers import SentenceTransformer

# í™˜ê²½ ì„¤ì •
from dotenv import load_dotenv


class BaseDocumentCompressor(ABC):
    """ì‚¬ìš©ì ì •ì˜ BaseDocumentCompressor - ì™„ì „íˆ ë…ë¦½ì """
    
    def __init__(self):
        pass
    
    @abstractmethod
    def compress_documents(
        self,
        documents: List[Document],
        query: str,
        callbacks: Optional[Any] = None,
    ) -> List[Document]:
        """ë¬¸ì„œ ì••ì¶•/ë¦¬ë­í‚¹ ë©”ì„œë“œ"""
        pass


class SentenceTransformerEmbeddings(Embeddings):
    """ê³µí†µ ì„ë² ë”© í´ë˜ìŠ¤"""
    def __init__(self, model_name: str = "intfloat/multilingual-e5-large-instruct"):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = self.model.encode(texts)
        return embeddings.tolist()

    def embed_query(self, text: str) -> List[float]:
        query_text = f"query: {text}"
        embedding = self.model.encode(query_text)
        return embedding.tolist()


class NaiveVectorStore(VectorStore):
    """ê³µí†µ NaiveVectorStore êµ¬í˜„"""
    def __init__(self, documents: List[Document], embeddings: List[List[float]], embedding_function: Embeddings):
        self.documents = documents
        self._embeddings_matrix = np.array(embeddings, dtype=np.float32)
        self.embedding_function = embedding_function
        self._embeddings_matrix = self._embeddings_matrix / np.linalg.norm(self._embeddings_matrix, axis=1, keepdims=True)

    def add_texts(self, texts: List[str], metadatas: Optional[List[dict]] = None, **kwargs) -> List[str]:
        raise NotImplementedError("add_textsëŠ” í˜„ì¬ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def similarity_search_by_vector(self, embedding: List[float], k: int = 4, **kwargs) -> List[Document]:
        query_vector = np.array(embedding, dtype=np.float32)
        query_norm = query_vector / np.linalg.norm(query_vector)
        similarities = np.dot(self._embeddings_matrix, query_norm)
        top_k_indices = similarities.argsort()[::-1][:k]
        return [self.documents[i] for i in top_k_indices]

    def similarity_search(self, query: str, k: int = 4, **kwargs) -> List[Document]:
        query_embedding = self.embedding_function.embed_query(query)
        return self.similarity_search_by_vector(query_embedding, k, **kwargs)

    def similarity_search_with_score(self, query: str, k: int = 4, **kwargs):
        query_embedding = self.embedding_function.embed_query(query)
        query_vector = np.array(query_embedding, dtype=np.float32)
        query_norm = query_vector / np.linalg.norm(query_vector)
        similarities = np.dot(self._embeddings_matrix, query_norm)
        top_k_indices = similarities.argsort()[::-1][:k]
        results = []
        for idx in top_k_indices:
            doc = self.documents[idx]
            score = float(similarities[idx])
            results.append((doc, score))
        return results

    def as_retriever(self, search_kwargs: Optional[dict] = None, **kwargs):
        """VectorStoreë¥¼ Retrieverë¡œ ë³€í™˜"""
        from langchain_core.retrievers import BaseRetriever
        from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun
        
        class NaiveRetriever(BaseRetriever):
            vectorstore: "NaiveVectorStore"
            search_kwargs: dict
            
            def __init__(self, vectorstore: "NaiveVectorStore", search_kwargs: dict = None, **kwargs):
                super().__init__(**kwargs)
                self.vectorstore = vectorstore
                self.search_kwargs = search_kwargs or {}
            
            def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None) -> List[Document]:
                k = self.search_kwargs.get("k", 4)
                return self.vectorstore.similarity_search(query, k=k)
        
        search_kwargs = search_kwargs or {}
        return NaiveRetriever(vectorstore=self, search_kwargs=search_kwargs, **kwargs)

    @classmethod
    def from_texts(cls, texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs):
        raise NotImplementedError("from_textsëŠ” í˜„ì¬ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


class SentenceTransformerRerank(BaseDocumentCompressor):
    """ì‚¬ìš©ì ì •ì˜ SentenceTransformerRerank - CrossEncoder ê¸°ë°˜"""
    
    def __init__(self, model_name: str, top_n: int = 10):
        super().__init__()
        try:
            from sentence_transformers import CrossEncoder
            self.model = CrossEncoder(model_name)
        except ImportError:
            raise ImportError("sentence_transformersê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sentence-transformers")
        self.top_n = top_n
    
    def compress_documents(
        self,
        documents: List[Document],
        query: str,
        callbacks: Optional[Any] = None,
    ) -> List[Document]:
        if not documents:
            return []
        
        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒìœ¼ë¡œ ì ìˆ˜ ê³„ì‚°
        pairs = [[query, doc.page_content] for doc in documents]
        scores = self.model.predict(pairs)
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        doc_scores = list(zip(documents, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ Nê°œ ë°˜í™˜
        return [doc for doc, score in doc_scores[:self.top_n]]


class SimpleRetriever:
    """ê°„ë‹¨í•œ Retriever êµ¬í˜„ - ContextualCompressionRetriever ëŒ€ì²´"""
    
    def __init__(self, base_retriever, compressor: BaseDocumentCompressor):
        self.base_retriever = base_retriever
        self.compressor = compressor
    
    def invoke(self, query: str) -> List[Document]:
        """ë¬¸ì„œ ê²€ìƒ‰ ë° ë¦¬ë­í‚¹"""
        # ê¸°ë³¸ ê²€ìƒ‰
        documents = self.base_retriever._get_relevant_documents(query)
        
        # ë¦¬ë­í‚¹
        if self.compressor and documents:
            documents = self.compressor.compress_documents(documents, query)
        
        return documents


def load_embeddings_data(embeddings_file: str) -> tuple:
    """ì„ë² ë”© ë°ì´í„° ë¡œë“œ ê³µí†µ í•¨ìˆ˜"""
    print(f"ğŸ“‚ ì„ë² ë”© ë°ì´í„° ë¡œë“œ ì¤‘: {embeddings_file}")
    with open(embeddings_file, "r", encoding="utf-8") as f:
        chunk_data = json.load(f)
    
    documents = []
    embeddings_array = []
    
    for item in chunk_data:
        doc = Document(
            page_content=item["text"],
            metadata={
                "filename": item["filename"],
                "chunk_index": item["chunk_index"],
                "source": f"{item['filename']}_chunk_{item['chunk_index']}"
            }
        )
        documents.append(doc)
        embeddings_array.append(item["embedding"])
    
    print(f"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    print(f"ğŸ“„ ì²« ë²ˆì§¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°: {documents[0].page_content[:100]}...")
    
    return documents, embeddings_array


def setup_environment():
    """í™˜ê²½ ì„¤ì • ê³µí†µ í•¨ìˆ˜"""
    load_dotenv()
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_ENDPOINT"] = os.getenv("LANGSMITH_ENDPOINT", "")
    os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY", "")
    print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ")


def create_legal_prompt():
    """ë²•ë¥  ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë¶€ë™ì‚°ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ğŸ“‹ **ì°¸ê³  ë¬¸ì„œ:**
{context}

ğŸ“ **ë‹µë³€ ì§€ì¹¨:**
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ê´€ë ¨ ë²•ë ¹ ì¡°ë¬¸ì´ë‚˜ ì¡°í•­ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”
3. ë²•ë¥ ì  ê·¼ê±°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”
4. ë¬¸ì„œì—ì„œ ëª…í™•í•œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”
5. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”

â“ **ì§ˆë¬¸:** {question}

ğŸ’¡ **ë‹µë³€:**"""
    )


def format_docs(docs):
    """ë¬¸ì„œ í¬ë§·íŒ… ê³µí†µ í•¨ìˆ˜"""
    formatted_docs = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
        content = doc.page_content.strip()
        formatted_docs.append(f"ğŸ“„ **ë¬¸ì„œ {i}** ({source})\n{content}")
    return "\n\n" + "\n\n".join(formatted_docs) + "\n\n"
