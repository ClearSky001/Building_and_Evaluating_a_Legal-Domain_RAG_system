{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAGAS ê¸°ë°˜ ë¦¬ë­ì»¤ ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ ë‹¤ì–‘í•œ ë¦¬ë­ì»¤ë“¤ì˜ ì„±ëŠ¥ì„ RAGAS ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.\n",
        "\n",
        "## í‰ê°€ ëŒ€ìƒ ë¦¬ë­ì»¤ë“¤\n",
        "- BM25 ê³„ì—´ (5ê°œ)\n",
        "- CrossEncoder ê³„ì—´ (7ê°œ)\n",
        "- BGE ê³„ì—´ (4ê°œ)\n",
        "- Embedding ê³„ì—´ (5ê°œ)\n",
        "- Hybrid ê³„ì—´ (4ê°œ)\n",
        "- LLM ê³„ì—´ (3ê°œ)\n",
        "- Rules ê³„ì—´ (1ê°œ)\n",
        "\n",
        "## RAGAS ë©”íŠ¸ë¦­\n",
        "- **Context Precision**: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •í™•ì„±\n",
        "- **Context Recall**: ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ì˜ ê²€ìƒ‰ ì™„ì„±ë„\n",
        "- **Faithfulness**: ìƒì„±ëœ ë‹µë³€ì˜ ì‹ ë¢°ì„±\n",
        "- **Answer Relevancy**: ë‹µë³€ì˜ ê´€ë ¨ì„±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
        "BASE_DIR = Path.cwd()\n",
        "RERANKERS_DIR = BASE_DIR / \"RAG_with_Various_Rerankers\"\n",
        "EMB_PATH = BASE_DIR / \"output_chunks_with_embeddings.json\"\n",
        "QA_PATH = BASE_DIR / \"real_estate_tax_QA.json\"\n",
        "\n",
        "# íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€\n",
        "sys.path.insert(0, str(RERANKERS_DIR))\n",
        "\n",
        "print(f\"BASE_DIR: {BASE_DIR}\")\n",
        "print(f\"RERANKERS_DIR: {RERANKERS_DIR}\")\n",
        "print(f\"EMB_PATH exists: {EMB_PATH.exists()}\")\n",
        "print(f\"QA_PATH exists: {QA_PATH.exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° import\n",
        "try:\n",
        "    import ragas\n",
        "    from ragas import evaluate\n",
        "    from ragas.metrics import (\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy\n",
        "    )\n",
        "    print(\"âœ… RAGAS íŒ¨í‚¤ì§€ê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
        "except ImportError:\n",
        "    print(\"ğŸ“¦ RAGAS íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ragas[all]\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì •ë‹µ ë°ì´í„° ë¡œë“œ ë° ë¶„ì„\n",
        "with open(QA_PATH, 'r', encoding='utf-8') as f:\n",
        "    qa_data = json.load(f)\n",
        "\n",
        "print(f\"ì´ ì§ˆë¬¸-ë‹µë³€ ìŒ: {len(qa_data)}ê°œ\")\n",
        "print(f\"ì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
        "print(f\"ì§ˆë¬¸: {qa_data[0]['question']}\")\n",
        "print(f\"ì •ë‹µ: {qa_data[0]['ground_truth']}\")\n",
        "print(f\"ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {len(qa_data[0]['ground_truth_contexts'])}\")\n",
        "print(f\"í† í”½: {qa_data[0]['metadata']['topic']}\")\n",
        "\n",
        "# í† í”½ë³„ ë¶„í¬ ë¶„ì„\n",
        "topic_counts = {}\n",
        "for item in qa_data:\n",
        "    topic = item['metadata']['topic']\n",
        "    topic_counts[topic] = topic_counts.get(topic, 0) + 1\n",
        "\n",
        "print(\"\\ní† í”½ë³„ ë¶„í¬:\")\n",
        "for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"  {topic}: {count}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í‰ê°€ìš© ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ (ì „ì²´ ë°ì´í„°ê°€ ë§ìœ¼ë¯€ë¡œ ìƒ˜í”Œë§)\n",
        "import random\n",
        "\n",
        "# ì¬í˜„ ê°€ëŠ¥í•œ ëœë¤ ìƒ˜í”Œë§\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# í† í”½ë³„ë¡œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§ (ê° í† í”½ë‹¹ ìµœëŒ€ 3ê°œ)\n",
        "sampled_data = []\n",
        "topic_samples = {}\n",
        "\n",
        "for item in qa_data:\n",
        "    topic = item['metadata']['topic']\n",
        "    if topic_samples.get(topic, 0) < 3:\n",
        "        sampled_data.append(item)\n",
        "        topic_samples[topic] = topic_samples.get(topic, 0) + 1\n",
        "\n",
        "# ìµœëŒ€ 30ê°œ ìƒ˜í”Œë¡œ ì œí•œ\n",
        "if len(sampled_data) > 30:\n",
        "    sampled_data = random.sample(sampled_data, 30)\n",
        "\n",
        "print(f\"í‰ê°€ìš© ìƒ˜í”Œ ë°ì´í„°: {len(sampled_data)}ê°œ\")\n",
        "print(\"ìƒ˜í”Œ í† í”½ ë¶„í¬:\")\n",
        "sample_topic_counts = {}\n",
        "for item in sampled_data:\n",
        "    topic = item['metadata']['topic']\n",
        "    sample_topic_counts[topic] = sample_topic_counts.get(topic, 0) + 1\n",
        "\n",
        "for topic, count in sorted(sample_topic_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"  {topic}: {count}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¦¬ë­ì»¤ ëª¨ë“ˆ ëª©ë¡ ì •ì˜\n",
        "RERANKER_MODULES = [\n",
        "    # BM25 ë¦¬ë­ì»¤ë“¤ (5ê°œ)\n",
        "    (\"BM25_Reranker.RAG_BM25_Rerank_FINAL\", \"BM25 ê¸°ë³¸\", \"LegalRAGSystemBM25Rerank\"),\n",
        "    (\"BM25_Reranker.RAG_BM25_CharNgram_Rerank_FINAL\", \"BM25 CharNgram\", \"LegalRAGSystemBM25CharNgram\"),\n",
        "    (\"BM25_Reranker.RAG_BM25_Kiwi_Rerank_FINAL\", \"BM25 Kiwi\", \"LegalRAGSystemBM25Kiwi\"),\n",
        "    (\"BM25_Reranker.RAG_BM25_Regex_Rerank_FINAL\", \"BM25 Regex\", \"LegalRAGSystemBM25Regex\"),\n",
        "    (\"BM25_Reranker.RAG_BM25_Stopword_Rerank_FINAL\", \"BM25 Stopword\", \"LegalRAGSystemBM25Stopword\"),\n",
        "    \n",
        "    # CrossEncoder ë¦¬ë­ì»¤ë“¤ (7ê°œ)\n",
        "    (\"CrossEncoder.RAG_CE_MiniLM_L6_Rerank_FINAL\", \"CrossEncoder MiniLM L6\", \"LegalRAGSystemMiniLML6\"),\n",
        "    (\"CrossEncoder.RAG_CE_MiniLM_L12_Rerank_FINAL\", \"CrossEncoder MiniLM L12\", \"LegalRAGSystemMiniLML12\"),\n",
        "    (\"CrossEncoder.RAG_CE_Electra_Rerank_FINAL\", \"CrossEncoder Electra\", \"LegalRAGSystemElectra\"),\n",
        "    (\"CrossEncoder.RAG_CE_E5_Mistral_Rerank_FINAL\", \"CrossEncoder E5 Mistral\", \"LegalRAGSystemE5Mistral\"),\n",
        "    (\"CrossEncoder.RAG_Cohere_Rerank_FINAL\", \"Cohere\", \"LegalRAGSystemCohere\"),\n",
        "    (\"CrossEncoder.RAG_MXBAI_Rerank_FINAL\", \"MXBAI\", \"LegalRAGSystemMXBAI\"),\n",
        "    \n",
        "    # BGE ê³„ì—´ (4ê°œ)\n",
        "    (\"CrossEncoder.BGE ê³„ì—´.RAG_BGE_Base_Rerank_FINAL\", \"BGE Base\", \"LegalRAGSystemBGEBase\"),\n",
        "    (\"CrossEncoder.BGE ê³„ì—´.RAG_BGE_Large_Rerank_FINAL\", \"BGE Large\", \"LegalRAGSystemBGELarge\"),\n",
        "    (\"CrossEncoder.BGE ê³„ì—´.RAG_BGE_v2m3_Rerank_FINAL\", \"BGE v2 M3\", \"LegalRAGSystemBGEv2m3\"),\n",
        "    (\"CrossEncoder.BGE ê³„ì—´.RAG_Flashrank_BGEv2m3_Rerank_FINAL\", \"Flashrank BGE v2m3\", \"LegalRAGSystemFlashrankBGEv2m3\"),\n",
        "    \n",
        "    # Embedding ë¦¬ë­ì»¤ë“¤ (5ê°œ)\n",
        "    (\"Embedding_Reranker.RAG_EmbeddingCosine_E5_Rerank_FINAL\", \"Embedding E5\", \"LegalRAGSystemEmbeddingE5\"),\n",
        "    (\"Embedding_Reranker.RAG_EmbeddingCosine_GTE_Rerank_FINAL\", \"Embedding GTE\", \"LegalRAGSystemEmbeddingGTE\"),\n",
        "    (\"Embedding_Reranker.RAG_EmbeddingCosine_MPNet_Rerank_FINAL\", \"Embedding MPNet\", \"LegalRAGSystemEmbeddingMPNet\"),\n",
        "    (\"Embedding_Reranker.RAG_EmbeddingCosine_Paraphrase_Rerank_FINAL\", \"Embedding Paraphrase\", \"LegalRAGSystemEmbeddingParaphrase\"),\n",
        "    (\"Embedding_Reranker.RAG_EmbeddingCosine_Stella_Rerank_FINAL\", \"Embedding Stella\", \"LegalRAGSystemEmbeddingStella\"),\n",
        "    \n",
        "    # Hybrid ë¦¬ë­ì»¤ë“¤ (4ê°œ)\n",
        "    (\"Hybrid_Reranker.RAG_CombSum_Rerank_FINAL\", \"Hybrid CombSum\", \"LegalRAGSystemCombSum\"),\n",
        "    (\"Hybrid_Reranker.RAG_CombMNZ_Rerank_FINAL\", \"Hybrid CombMNZ\", \"LegalRAGSystemCombMNZ\"),\n",
        "    (\"Hybrid_Reranker.RAG_RRF_Rerank_FINAL\", \"Hybrid RRF\", \"LegalRAGSystemRRF\"),\n",
        "    (\"Hybrid_Reranker.RAG_WeightedSum_BM25_Embed_Rerank_FINAL\", \"Hybrid WeightedSum\", \"LegalRAGSystemWeightedSum\"),\n",
        "    \n",
        "    # LLM ë¦¬ë­ì»¤ë“¤ (3ê°œ)\n",
        "    (\"LLM_Reranker.RAG_LLM_Rerank_FINAL\", \"LLM ê¸°ë³¸\", \"LegalRAGSystemLLM\"),\n",
        "    (\"LLM_Reranker.RAG_LLM_Listwise_Rerank_FINAL\", \"LLM Listwise\", \"LegalRAGSystemLLMListwise\"),\n",
        "    (\"LLM_Reranker.RAG_LLM_Pairwise_Rerank_FINAL\", \"LLM Pairwise\", \"LegalRAGSystemLLMPairwise\"),\n",
        "    \n",
        "    # Rules ë¦¬ë­ì»¤ë“¤ (1ê°œ)\n",
        "    (\"Rules_Reranker.RAG_LegalRuleBoost_Rerank_FINAL\", \"Legal Rule Boost\", \"LegalRAGSystemLegalRuleBoost\")\n",
        "]\n",
        "\n",
        "print(f\"ì´ {len(RERANKER_MODULES)}ê°œì˜ ë¦¬ë­ì»¤ ëª¨ë“ˆì„ í‰ê°€í•©ë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¦¬ë­ì»¤ ë¡œë“œ ë° ì´ˆê¸°í™” í•¨ìˆ˜\n",
        "def load_reranker(module_path: str, class_name: str) -> Any:\n",
        "    \"\"\"ë¦¬ë­ì»¤ ëª¨ë“ˆì„ ë™ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    try:\n",
        "        module = __import__(module_path, fromlist=[class_name])\n",
        "        reranker_class = getattr(module, class_name)\n",
        "        return reranker_class(embeddings_file=str(EMB_PATH))\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ {module_path} ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "        return None\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ìš© ë¦¬ë­ì»¤ ë¡œë“œ (ì²˜ìŒ ëª‡ ê°œë§Œ)\n",
        "test_rerankers = {}\n",
        "for module_path, name, class_name in RERANKER_MODULES[:3]:  # ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸\n",
        "    print(f\"ë¡œë”© ì¤‘: {name}...\")\n",
        "    reranker = load_reranker(module_path, class_name)\n",
        "    if reranker:\n",
        "        test_rerankers[name] = reranker\n",
        "        print(f\"âœ… {name} ë¡œë“œ ì„±ê³µ\")\n",
        "    else:\n",
        "        print(f\"âŒ {name} ë¡œë“œ ì‹¤íŒ¨\")\n",
        "\n",
        "print(f\"\\nì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ ë¦¬ë­ì»¤: {len(test_rerankers)}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGAS í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜\n",
        "def prepare_ragas_data(sampled_data: List[Dict], reranker: Any, reranker_name: str) -> List[Dict]:\n",
        "    \"\"\"RAGAS í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\"\"\"\n",
        "    ragas_data = []\n",
        "    \n",
        "    for i, item in enumerate(sampled_data):\n",
        "        try:\n",
        "            question = item['question']\n",
        "            ground_truth = item['ground_truth']\n",
        "            ground_truth_contexts = item['ground_truth_contexts']\n",
        "            \n",
        "            # ë¦¬ë­ì»¤ë¡œ ë¬¸ì„œ ê²€ìƒ‰\n",
        "            docs = reranker.retriever.invoke(question)\n",
        "            \n",
        "            # RAGAS í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "            ragas_item = {\n",
        "                'question': question,\n",
        "                'answer': ground_truth,  # ì •ë‹µì„ ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©\n",
        "                'contexts': [doc.page_content for doc in docs],\n",
        "                'ground_truth': ground_truth,\n",
        "                'ground_truths': ground_truth_contexts\n",
        "            }\n",
        "            ragas_data.append(ragas_item)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {reranker_name} - ì§ˆë¬¸ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return ragas_data\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
        "print(\"RAGAS í‰ê°€ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
        "test_ragas_data = {}\n",
        "\n",
        "for name, reranker in test_rerankers.items():\n",
        "    print(f\"\\n{name} ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
        "    ragas_data = prepare_ragas_data(sampled_data, reranker, name)\n",
        "    test_ragas_data[name] = ragas_data\n",
        "    print(f\"âœ… {name}: {len(ragas_data)}ê°œ ìƒ˜í”Œ ì¤€ë¹„ ì™„ë£Œ\")\n",
        "\n",
        "print(f\"\\nì´ {len(test_ragas_data)}ê°œ ë¦¬ë­ì»¤ì˜ RAGAS ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGAS í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜\n",
        "def evaluate_with_ragas(ragas_data: List[Dict], reranker_name: str) -> Dict[str, float]:\n",
        "    \"\"\"RAGAS ë©”íŠ¸ë¦­ìœ¼ë¡œ ë¦¬ë­ì»¤ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.\"\"\"\n",
        "    try:\n",
        "        # DataFrameìœ¼ë¡œ ë³€í™˜\n",
        "        df = pd.DataFrame(ragas_data)\n",
        "        \n",
        "        # RAGAS í‰ê°€ ì‹¤í–‰\n",
        "        metrics = [\n",
        "            context_precision,\n",
        "            context_recall,\n",
        "            faithfulness,\n",
        "            answer_relevancy\n",
        "        ]\n",
        "        \n",
        "        result = evaluate(df, metrics=metrics)\n",
        "        \n",
        "        # ê²°ê³¼ ì¶”ì¶œ\n",
        "        scores = {\n",
        "            'context_precision': float(result['context_precision']),\n",
        "            'context_recall': float(result['context_recall']),\n",
        "            'faithfulness': float(result['faithfulness']),\n",
        "            'answer_relevancy': float(result['answer_relevancy'])\n",
        "        }\n",
        "        \n",
        "        print(f\"âœ… {reranker_name} í‰ê°€ ì™„ë£Œ\")\n",
        "        for metric, score in scores.items():\n",
        "            print(f\"  {metric}: {score:.4f}\")\n",
        "        \n",
        "        return scores\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ {reranker_name} í‰ê°€ ì‹¤íŒ¨: {e}\")\n",
        "        return {\n",
        "            'context_precision': 0.0,\n",
        "            'context_recall': 0.0,\n",
        "            'faithfulness': 0.0,\n",
        "            'answer_relevancy': 0.0\n",
        "        }\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë¦¬ë­ì»¤ë“¤ í‰ê°€\n",
        "print(\"RAGAS í‰ê°€ ì‹œì‘...\")\n",
        "evaluation_results = {}\n",
        "\n",
        "for name, ragas_data in test_ragas_data.items():\n",
        "    print(f\"\\n{name} í‰ê°€ ì¤‘...\")\n",
        "    scores = evaluate_with_ragas(ragas_data, name)\n",
        "    evaluation_results[name] = scores\n",
        "\n",
        "print(\"\\nğŸ‰ í‰ê°€ ì™„ë£Œ!\")\n",
        "print(f\"í‰ê°€ëœ ë¦¬ë­ì»¤ ìˆ˜: {len(evaluation_results)}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
        "results_df = pd.DataFrame(evaluation_results).T\n",
        "results_df = results_df.round(4)\n",
        "\n",
        "# ì¢…í•© ì ìˆ˜ ê³„ì‚° (í‰ê· )\n",
        "results_df['overall_score'] = results_df.mean(axis=1)\n",
        "results_df = results_df.sort_values('overall_score', ascending=False)\n",
        "\n",
        "print(\"ğŸ“Š RAGAS í‰ê°€ ê²°ê³¼ (ìƒìœ„ 10ê°œ)\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.head(10))\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥\n",
        "results_df.to_csv('ragas_evaluation_results.csv', encoding='utf-8-sig')\n",
        "print(\"\\nğŸ’¾ ê²°ê³¼ê°€ 'ragas_evaluation_results.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‹œê°í™” 1: ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ë¹„êµ\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# ì„œë¸Œí”Œë¡¯ ì„¤ì •\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('RAGAS ë©”íŠ¸ë¦­ë³„ ë¦¬ë­ì»¤ ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics = ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']\n",
        "metric_names = ['Context Precision', 'Context Recall', 'Faithfulness', 'Answer Relevancy']\n",
        "\n",
        "for i, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
        "    ax = axes[i//2, i%2]\n",
        "    \n",
        "    # ë°ì´í„° ì¤€ë¹„\n",
        "    data = results_df[metric].sort_values(ascending=True)\n",
        "    \n",
        "    # ë§‰ëŒ€ ê·¸ë˜í”„\n",
        "    bars = ax.barh(range(len(data)), data.values, color='skyblue', alpha=0.7)\n",
        "    \n",
        "    # ë ˆì´ë¸” ì„¤ì •\n",
        "    ax.set_yticks(range(len(data)))\n",
        "    ax.set_yticklabels(data.index, fontsize=8)\n",
        "    ax.set_xlabel(name, fontsize=10)\n",
        "    ax.set_title(f'{name} ë¹„êµ', fontsize=12, fontweight='bold')\n",
        "    \n",
        "    # ê°’ í‘œì‹œ\n",
        "    for j, bar in enumerate(bars):\n",
        "        width = bar.get_width()\n",
        "        ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "                f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
        "    \n",
        "    # ê·¸ë¦¬ë“œ ì¶”ê°€\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    ax.set_xlim(0, 1.0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ragas_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‹œê°í™” 2: ì¢…í•© ì„±ëŠ¥ ìˆœìœ„\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# ì¢…í•© ì ìˆ˜ ê¸°ì¤€ ì •ë ¬\n",
        "overall_scores = results_df['overall_score'].sort_values(ascending=True)\n",
        "\n",
        "# ë§‰ëŒ€ ê·¸ë˜í”„\n",
        "bars = plt.barh(range(len(overall_scores)), overall_scores.values, \n",
        "                color='lightcoral', alpha=0.8, edgecolor='darkred')\n",
        "\n",
        "# ë ˆì´ë¸” ì„¤ì •\n",
        "plt.yticks(range(len(overall_scores)), overall_scores.index)\n",
        "plt.xlabel('ì¢…í•© ì ìˆ˜ (Overall Score)', fontsize=12)\n",
        "plt.title('ë¦¬ë­ì»¤ ì¢…í•© ì„±ëŠ¥ ìˆœìœ„ (RAGAS ê¸°ì¤€)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# ê°’ í‘œì‹œ\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()\n",
        "    plt.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "             f'{width:.3f}', ha='left', va='center', fontsize=10)\n",
        "\n",
        "# ê·¸ë¦¬ë“œ ì¶”ê°€\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.xlim(0, 1.0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('reranker_overall_ranking.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nğŸ† ìƒìœ„ 5ê°œ ë¦¬ë­ì»¤:\")\n",
        "for i, (name, score) in enumerate(overall_scores.tail(5).items()):\n",
        "    print(f\"{i+1}. {name}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‹œê°í™” 3: íˆíŠ¸ë§µ - ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ì„±ëŠ¥\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„\n",
        "heatmap_data = results_df[['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']]\n",
        "\n",
        "# íˆíŠ¸ë§µ ìƒì„±\n",
        "sns.heatmap(heatmap_data, \n",
        "            annot=True, \n",
        "            cmap='YlOrRd', \n",
        "            fmt='.3f',\n",
        "            cbar_kws={'label': 'ì ìˆ˜'},\n",
        "            annot_kws={'size': 8})\n",
        "\n",
        "plt.title('ë¦¬ë­ì»¤ë³„ RAGAS ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('RAGAS ë©”íŠ¸ë¦­', fontsize=12)\n",
        "plt.ylabel('ë¦¬ë­ì»¤', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ragas_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìƒì„¸ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­\n",
        "print(\"ğŸ” ìƒì„¸ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ìµœê³  ì„±ëŠ¥ ë¦¬ë­ì»¤\n",
        "best_reranker = results_df.index[0]\n",
        "best_score = results_df.iloc[0]['overall_score']\n",
        "print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ë¦¬ë­ì»¤: {best_reranker}\")\n",
        "print(f\"   ì¢…í•© ì ìˆ˜: {best_score:.4f}\")\n",
        "\n",
        "# ë©”íŠ¸ë¦­ë³„ ìµœê³  ì„±ëŠ¥\n",
        "print(\"\\nğŸ“Š ë©”íŠ¸ë¦­ë³„ ìµœê³  ì„±ëŠ¥:\")\n",
        "for metric in ['context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']:\n",
        "    best_idx = results_df[metric].idxmax()\n",
        "    best_value = results_df[metric].max()\n",
        "    print(f\"   {metric}: {best_idx} ({best_value:.4f})\")\n",
        "\n",
        "# ì„±ëŠ¥ ë¶„ì„\n",
        "print(\"\\nğŸ“ˆ ì„±ëŠ¥ ë¶„ì„:\")\n",
        "avg_scores = results_df.mean()\n",
        "print(f\"   ì „ì²´ í‰ê·  ì¢…í•© ì ìˆ˜: {avg_scores['overall_score']:.4f}\")\n",
        "print(f\"   Context Precision í‰ê· : {avg_scores['context_precision']:.4f}\")\n",
        "print(f\"   Context Recall í‰ê· : {avg_scores['context_recall']:.4f}\")\n",
        "print(f\"   Faithfulness í‰ê· : {avg_scores['faithfulness']:.4f}\")\n",
        "print(f\"   Answer Relevancy í‰ê· : {avg_scores['answer_relevancy']:.4f}\")\n",
        "\n",
        "# ê¶Œì¥ì‚¬í•­\n",
        "print(\"\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:\")\n",
        "if best_score > 0.8:\n",
        "    print(\"   âœ… ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ë¦¬ë­ì»¤ê°€ ìˆìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(\"   âš ï¸  ì „ì²´ì ì¸ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "if avg_scores['context_precision'] < 0.7:\n",
        "    print(\"   ğŸ“ Context Precision ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "if avg_scores['context_recall'] < 0.7:\n",
        "    print(\"   ğŸ“ Context Recall ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "if avg_scores['faithfulness'] < 0.7:\n",
        "    print(\"   ğŸ“ Faithfulness ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "if avg_scores['answer_relevancy'] < 0.7:\n",
        "    print(\"   ğŸ“ Answer Relevancy ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì „ì²´ ë¦¬ë­ì»¤ í‰ê°€ ì‹¤í–‰ (ì„ íƒì‚¬í•­)\n",
        "print(\"ğŸš€ ì „ì²´ ë¦¬ë­ì»¤ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\")\n",
        "print(\"âš ï¸  ì£¼ì˜: ì´ ì‘ì—…ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "print(\"ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ì…€ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "\n",
        "# ì „ì²´ í‰ê°€ ì½”ë“œ (ì£¼ì„ ì²˜ë¦¬)\n",
        "\"\"\"\n",
        "# ì „ì²´ ë¦¬ë­ì»¤ ë¡œë“œ ë° í‰ê°€\n",
        "print(\"ì „ì²´ ë¦¬ë­ì»¤ ë¡œë”© ì‹œì‘...\")\n",
        "all_rerankers = {}\n",
        "\n",
        "for module_path, name, class_name in RERANKER_MODULES:\n",
        "    print(f\"ë¡œë”© ì¤‘: {name}...\")\n",
        "    reranker = load_reranker(module_path, class_name)\n",
        "    if reranker:\n",
        "        all_rerankers[name] = reranker\n",
        "        print(f\"âœ… {name} ë¡œë“œ ì„±ê³µ\")\n",
        "    else:\n",
        "        print(f\"âŒ {name} ë¡œë“œ ì‹¤íŒ¨\")\n",
        "\n",
        "print(f\"\\nì „ì²´ ë¡œë“œëœ ë¦¬ë­ì»¤: {len(all_rerankers)}ê°œ\")\n",
        "\n",
        "# ì „ì²´ RAGAS ë°ì´í„° ì¤€ë¹„\n",
        "print(\"\\nì „ì²´ RAGAS ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
        "all_ragas_data = {}\n",
        "\n",
        "for name, reranker in all_rerankers.items():\n",
        "    print(f\"\\n{name} ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
        "    ragas_data = prepare_ragas_data(sampled_data, reranker, name)\n",
        "    all_ragas_data[name] = ragas_data\n",
        "    print(f\"âœ… {name}: {len(ragas_data)}ê°œ ìƒ˜í”Œ ì¤€ë¹„ ì™„ë£Œ\")\n",
        "\n",
        "# ì „ì²´ í‰ê°€ ì‹¤í–‰\n",
        "print(\"\\nì „ì²´ RAGAS í‰ê°€ ì‹œì‘...\")\n",
        "all_evaluation_results = {}\n",
        "\n",
        "for name, ragas_data in all_ragas_data.items():\n",
        "    print(f\"\\n{name} í‰ê°€ ì¤‘...\")\n",
        "    scores = evaluate_with_ragas(ragas_data, name)\n",
        "    all_evaluation_results[name] = scores\n",
        "\n",
        "print(\"\\nğŸ‰ ì „ì²´ í‰ê°€ ì™„ë£Œ!\")\n",
        "\n",
        "# ì „ì²´ ê²°ê³¼ ì €ì¥\n",
        "all_results_df = pd.DataFrame(all_evaluation_results).T\n",
        "all_results_df['overall_score'] = all_results_df.mean(axis=1)\n",
        "all_results_df = all_results_df.sort_values('overall_score', ascending=False)\n",
        "all_results_df.to_csv('all_ragas_evaluation_results.csv', encoding='utf-8-sig')\n",
        "print(\"ì „ì²´ ê²°ê³¼ê°€ 'all_ragas_evaluation_results.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
